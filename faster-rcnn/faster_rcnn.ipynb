{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpatches\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpatches\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "from utils import *\n",
    "from model import *\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import ops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetecionDataset(Dataset):\n",
    "    def __init__(self, annotation_path, img_dir, img_size, name2idx) -> None:\n",
    "        self.annotation_path = annotation_path\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "        self.name2idx = name2idx\n",
    "        \n",
    "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_data_all.size(dim=0)\n",
    "    \n",
    "    def __getitem__(self, idx) -> T_co:\n",
    "        return self.img_data_all[idx], self.gt_bboxes_all[idx], self.gt_classes_all[idx]\n",
    "    \n",
    "    def get_data(self):\n",
    "        self.img_data_all = []\n",
    "        self.gt_idxs_all = []\n",
    "        \n",
    "        gt_boxes_all, gt_classes_all, img_paths = parse_annotations(self.annotation_path, self.img_dir, self.img_size)\n",
    "        \n",
    "        for i, img_path in enumerate(img_paths):\n",
    "            \n",
    "            if (not img_path) or (not os.path.exists(img_path)):\n",
    "                continue \n",
    "            \n",
    "            img = io.imread(img_path)\n",
    "            img = resize(img, self.img_size)\n",
    "            \n",
    "            # convert image to torch and reshape it so channels come first\n",
    "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            \n",
    "            # encode class names as integers \n",
    "            gt_classes = gt_classes[i]\n",
    "            gt_idx = torch.Tensor([self.name2idx[name] for name in gt_classes])\n",
    "            \n",
    "            img_data_all.append(img_tensor)\n",
    "            gt_idxs_all.append(gt_idx)\n",
    "            \n",
    "        # pad bounding boxes and classes so they are of the same size\n",
    "        gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1)\n",
    "        gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        # stack all images\n",
    "        img_data_stacked = torch.stack(img_data_all, dim=0)\n",
    "        \n",
    "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "req_layers = list(model.children())[:8]\n",
    "backbone = nn.Sequential(*req_layers)\n",
    "\n",
    "# unfreeze all the parameters\n",
    "for param in backbone.named_parameters():\n",
    "    param[1].requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_anc_centers(out_size):\n",
    "    out_h, out_w = out_size\n",
    "    \n",
    "    anc_pts_x = torch.arange(0, out_w) + 0.5\n",
    "    anc_pts_y = torch.arange(0, out_h) + 0.5\n",
    "    \n",
    "    return anc_pts_x, anc_pts_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize anchor points onto the image\n",
    "anc_pts_x_vis = anc_pts_x.clone() * width_scale_factor\n",
    "anc_pts_y_vis = anc_pts_y.clone() * height_scale_factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Anchor Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_scales = [2, 4, 6]\n",
    "anc_ratios = [0.5, 1, 1.5]\n",
    "n_anc_boxes = len(anc_scales) * len(anc_ratios) # no. of anchor boxes for each anchor point\n",
    "\n",
    "anc_base = gen_anc_base(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_anc_base(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
    "    n_anc_boxes = len(anc_scales) * len(anc_ratios)\n",
    "    anc_base = torch.zeros(\n",
    "        1, \n",
    "        anc_pts_x.size(dim=0),\n",
    "        anc_pts_y.size(dim=0),\n",
    "        n_anc_boxes,\n",
    "        4\n",
    "        ) # shape [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
    "    \n",
    "    for ix, xc in enumerate(anc_pts_x):\n",
    "        for jx, yc in enumerate(anc_pts_y):\n",
    "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
    "            c = 0\n",
    "            for i, scale in enumerate(anc_scales):\n",
    "                for j, ratio in enumerate(anc_ratios):\n",
    "                    w = scale * ratio\n",
    "                    h = scale\n",
    "                    \n",
    "                    xmin = xc - w / 2\n",
    "                    ymin = yc - h / 2\n",
    "                    xmax = xc + w / 2\n",
    "                    ymax = yc + h / 2\n",
    "                    \n",
    "                    anc_base[:, ix, jx, :] = ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
    "                    \n",
    "    return anc_base\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the positive anchor boxes which have iou > 0.7 with any of ground truth\n",
    "# or those that have highest iou for every ground truth\n",
    "\n",
    "def get_io_mat(batch_size, anc_boxes_all, gt_bboxes_all):\n",
    "    \n",
    "    # flatten anchor boxes\n",
    "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
    "    # get total anchor boxes for a single image\n",
    "    tot_anc_boxes = anc_boxes_flat.size(dim=1)\n",
    "    \n",
    "    # create a placeholder to compute IoUs amongst the boxes\n",
    "    ious_mat = torch.zeros((batch_size, tot_anc_boxes, gt_bboxes_all.size(dim=1)))\n",
    "    \n",
    "    # compute the IoU of the anc boxes with the gt boxes for all the images\n",
    "    for i in range(batch_size):\n",
    "        gt_bboxes = gt_bboxes_all[i]\n",
    "        anc_boxes = anc_boxes_flat[i]\n",
    "        ious_mat[i, :] = ops.box_iou(anc_boxes, gt_bboxes)\n",
    "        \n",
    "    return ious_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode=\"a2p\"):\n",
    "    assert mode in [\"a2p\", \"p2a\"]\n",
    "    \n",
    "    batch_size = bboxes.size(dim=0)\n",
    "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
    "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
    "    \n",
    "    if mode == \"a2p\":\n",
    "        # activation map to pixel image\n",
    "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor\n",
    "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor\n",
    "    else:\n",
    "        # pixel image to activation map\n",
    "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
    "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
    "        \n",
    "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
    "    proj_bboxes.resize_as_(bboxes)\n",
    "    \n",
    "    return proj_bboxes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gt_offsets(pos_anc_coords, gt_bbox_mapping):\n",
    "    pos_anc_coords = ops.box_convert(pos_anc_coords, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "    gt_bbox_mapping = ops.box_convert(gt_bbox_mapping, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "    \n",
    "    gt_cx, gt_cy, gt_w, gt_h = gt_bbox_mapping[:, 0], gt_bbox_mapping[:, 1], gt_bbox_mapping[:, 2], gt_bbox_mapping[:, 3]\n",
    "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
    "    \n",
    "    tx_ = (gt_cx - anc_cx) / anc_w\n",
    "    ty_ = (gt_cy - anc_cy) / anc_h\n",
    "    tw_ = torch.log(gt_w / anc_w)\n",
    "    th_ = torch.log(gt_h / anc_h)\n",
    "    \n",
    "    # gt_cx, gt_cy = centers of ground truth boxes\n",
    "    # anc_cx, anc_cy = centers of anchor boxes\n",
    "    # gt_w, gt_h = width and height of ground truth boxes\n",
    "    # anc_w, anc_h = width and height of anchor boxes\n",
    "    \n",
    "    return torch.stack([tx_, ty_, th_], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_req_anchors(anc_boxes_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
